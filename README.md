<div align="center">


# RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility

[![arXiv](https://img.shields.io/badge/arXiv-RHYTHM-ff0000.svg?style=for-the-badge)](https://arxiv.org/abs/2509.23115)  [![Github](https://img.shields.io/badge/RHYTHM-000000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/he-h/rhythm)
</div>




The official implementation of [**RHYTHM**: Reasoning with Hierarchical Temporal Tokenization for Human Mobility](https://arxiv.org/abs/2509.23115).

## Contents

- [**RHYTHM**: Reasoning with Hierarchical Temporal Tokenization for Human Mobility](https://arxiv.org/abs/2509.23115) 
  - [1. Introduction](#1)
  - [2. Prerequisites](#2-prerequisites)
  - [3. Data](#3-data)
  - [4. Usage](#4-usage)
    - [4.1 Preprocess](#41-preprocess)
    - [4.2 Training](#42-training)
    - [4.3 Evaluate](#43-evaluate)
  - [5. Acknowledgement](#5-acknowledgement)
  - [6. Citation](#6-citation)
  - [7. Contact](#7-contact)
 


## Introduction
**RHYTHM** (Reasoning with Hierarchical Temporal Tokenization for Human Mobility) is a novel framework that recasts mobility modeling as a temporal-token reasoning task. By segmenting trajectories into daily tokens (and pooling them hierarchically), RHYTHM dramatically shortens sequence length while retaining cyclical structure. It then enriches those tokens with semantic embeddings generated by a frozen LLM, and feeds them into the same model as a reasoning backbone. This combination of compressed temporal abstraction + LLM-based reasoning delivers stronger predictive accuracy with far lower computational cost than prior methods.






## Prerequisites

- Python 3.10+
- PyTorch
- Hugging Face Transformers
- Other dependencies listed in `requirements.txt`

```
# create and activate virtual python environment
conda create -n rhythm python=3.10
conda activate rhythm
pip install transformers
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126

# install required packages
pip install -r requirements.txt
```

## Data

Our experiments are conducted on the [YJMob100k](https://www.nature.com/articles/s43588-024-00650-3) dataset.

## Usage



### 1. Preprocess

First, preprocess your raw trajectory files and compute semantic embeddings offline:

```bash
# Preprocess raw data
bash scripts/preprocess.sh
```

### 2. Training
Trained model outputs, logs, and final metrics are saved under the log/ directory, and checkpoints are saved under the checkpoints/ directory.
You can train the model on different cities by running the following scripts:

```bash
# Train on City B
bash scripts/b.sh

# Train on City C
bash scripts/c.sh

# Train on City D
bash scripts/d.sh
```

### 3. Evaluate
Coming soon...

## Acknowledgement

We appreciate the following GitHub repos a lot for their valuable code and efforts.
- Time-Series-Library (https://github.com/thuml/Time-Series-Library)
- AutoTimes (https://github.com/thuml/AutoTimes)
- ST-MoE-BERT (https://github.com/he-h/ST-MoE-BERT)



## Citation

If you have any questions regarding our paper or code, please feel free to start an issue.

If you use RHYTHM in your work, please kindly cite our paper:

```
@misc{he2025rhythmreasoninghierarchicaltemporal,
      title={RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility}, 
      author={Haoyu He and Haozheng Luo and Yan Chen and Qi R. Wang},
      year={2025},
      eprint={2509.23115},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2509.23115}, 
}
```

## Contact

If you have any questions or want to use the code, feel free to contact:
* Haoyu He (he.haoyu1@northeastern.edu)
* Robin Luo (hluo@u.northwestern.edu)
